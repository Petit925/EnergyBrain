import os
import fitz  # PyMuPDF
import tiktoken
from dotenv import load_dotenv
from openai import OpenAI
from pinecone import Pinecone

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–ª—é—á—ñ–≤
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENVIRONMENT = os.getenv("PINECONE_ENVIRONMENT")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME")

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª—ñ—î–Ω—Ç—ñ–≤
client = OpenAI(api_key=OPENAI_API_KEY)
pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index(PINECONE_INDEX_NAME)

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è PDF —ñ —Ä–æ–∑–±–∏—Ç—Ç—è –Ω–∞ —á–∞–Ω–∫–∏
def load_pdf_text(path):
    doc = fitz.open(path)
    return "\n".join([page.get_text() for page in doc])

def chunk_text(text, max_tokens=500):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)
    chunks = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]
    return [tokenizer.decode(chunk) for chunk in chunks]

# –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤
def embed_texts(texts):
    embeddings = []
    for chunk in texts:
        res = client.embeddings.create(
            input=[chunk],
            model="text-embedding-ada-002"
        )
        embeddings.append((chunk, res.data[0].embedding))
    return embeddings

# –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤ Pinecone
def upload_to_pinecone(embeddings):
    vectors = [
        {"id": f"chunk-{i}", "values": embedding, "metadata": {"text": chunk}}
        for i, (chunk, embedding) in enumerate(embeddings)
    ]
    index.upsert(vectors=vectors)

# –ü–æ—à—É–∫
def search_index(query, top_k=5):
    res = client.embeddings.create(input=[query], model="text-embedding-ada-002")
    query_embed = res.data[0].embedding
    result = index.query(vector=query_embed, top_k=top_k, include_metadata=True)
    return result['matches']

# –ü–æ–±—É–¥–æ–≤–∞ prompt —ñ GPT-–≤—ñ–¥–ø–æ–≤—ñ–¥—å
def build_prompt(query, results):
    context = "\n---\n".join([r["metadata"]["text"] for r in results])
    return f"""
–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ –∫–æ–º–ø–ª–∞—î–Ω—Å—É. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –∫–æ—Ä–æ—Ç–∫–æ, —á—ñ—Ç–∫–æ, —Ç—ñ–ª—å–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–¥–∞–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–ó–∞–ø–∏—Ç: {query}
–í—ñ–¥–ø–æ–≤—ñ–¥—å:
"""

def ask_gpt(prompt):
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content

# –û—Å–Ω–æ–≤–Ω–∏–π —Å–∫—Ä–∏–ø—Ç
def main():
    pdf_path = "compliance_manual.pdf"  # –ù–∞–∑–≤–∞ —Ç–≤–æ–≥–æ PDF-—Ñ–∞–π–ª—É

    print("üîç –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç–∞...")
    full_text = load_pdf_text(pdf_path)
    chunks = chunk_text(full_text)
    
    print(f"üîó –°—Ç–≤–æ—Ä–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫—ñ–≤. –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤...")
    embeddings = embed_texts(chunks)

    print("‚¨ÜÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —É Pinecone...")
    upload_to_pinecone(embeddings)

    while True:
        query = input("\nüìù –í–≤–µ–¥—ñ—Ç—å –∑–∞–ø–∏—Ç (–∞–±–æ 'exit'): ")
        if query.lower() == 'exit':
            break
        matches = search_index(query)
        prompt = build_prompt(query, matches)
        response = ask_gpt(prompt)
        print(f"\nüí° GPT-–≤—ñ–¥–ø–æ–≤—ñ–¥—å:\n{response}")

if __name__ == "__main__":
    main()
