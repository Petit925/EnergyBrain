import fitz  # PyMuPDF
import tiktoken
import streamlit as st
from openai import OpenAI
import pinecone

# üîê –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è OpenAI —Ç–∞ Pinecone
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
pinecone.init(
    api_key=st.secrets["PINECONE_API_KEY"],
    environment=st.secrets["PINECONE_ENVIRONMENT"]
)

# ---------- –†–æ–±–æ—Ç–∞ –∑ PDF ----------
def load_pdf_text(path):
    doc = fitz.open(path)
    return "\n".join([page.get_text() for page in doc])

# ---------- –†–æ–∑–±–∏—Ç—Ç—è —Ç–µ–∫—Å—Ç—É ----------
def chunk_text(text, max_tokens=500):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)
    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]
    return [tokenizer.decode(chunk) for chunk in chunks]

# ---------- –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –µ–º–±–µ–¥—ñ–Ω–≥—ñ–≤ ----------
def embed_texts(texts):
    embeddings = []
    for chunk in texts:
        res = client.embeddings.create(input=[chunk], model="text-embedding-ada-002")
        embeddings.append((chunk, res.data[0].embedding))
    return embeddings

# ---------- –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —É Pinecone ----------
def upload_to_pinecone(embeddings, index_name):
    index = pinecone.Index(index_name)
    vectors = [
        {
            "id": f"chunk-{i}",
            "values": embedding,
            "metadata": {"text": chunk}
        }
        for i, (chunk, embedding) in enumerate(embeddings)
    ]
    index.upsert(vectors=vectors)

# ---------- –ü–æ—à—É–∫ —É Pinecone ----------
def search_index(query, top_k=5, index_name=None, source_filter=None):
    if not index_name:
        index_name = st.secrets["PINECONE_INDEX_NAME"]
    index = pinecone.Index(index_name)

    res = client.embeddings.create(input=[query], model="text-embedding-ada-002")
    query_embed = res.data[0].embedding

    filter_obj = {"source": source_filter} if source_filter else {}

    result = index.query(
        vector=query_embed,
        top_k=top_k,
        include_metadata=True,
        filter=filter_obj
    )
    return result["matches"]

# ---------- –ü–æ–±—É–¥–æ–≤–∞ –∑–∞–ø–∏—Ç—É –¥–ª—è GPT ----------
def build_prompt(query, results):
    context = "\n---\n".join([r["metadata"]["text"] for r in results])
    return f"""
–¢–∏ –µ–∫—Å–ø–µ—Ä—Ç –∑ –ø—É–±–ª—ñ—á–Ω–∏—Ö –∑–∞–∫—É–ø—ñ–≤–µ–ª—å. –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –∫–æ—Ä–æ—Ç–∫–æ, —á—ñ—Ç–∫–æ, —Ç—ñ–ª—å–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –Ω–∞–¥–∞–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–ó–∞–ø–∏—Ç: {query}
–í—ñ–¥–ø–æ–≤—ñ–¥—å:
"""

# ---------- –í–∏–∫–ª–∏–∫ GPT ----------
def ask_gpt(prompt):
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}]
    )
    return response.choices[0].message.content
